{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4. Capstone - Model Selectio.ipynb","provenance":[],"collapsed_sections":["hCbgZU6Ata9m"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"10866add10fc4adb891eba667a875352":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a76f0874e9ad4632969bab26eafb0c28","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a2ad146acdd24d3f9547d156889d7de3","IPY_MODEL_1725c42bac7842699d8e2a0e01a1b28f"]}},"a76f0874e9ad4632969bab26eafb0c28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a2ad146acdd24d3f9547d156889d7de3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_12fb34a724574d298295d03b34f95ab9","_dom_classes":[],"description":"Optimization Progress: ","_model_name":"FloatProgressModel","bar_style":"","max":1515,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1515,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bfb558a473e34d399bf5f25f9b49089d"}},"1725c42bac7842699d8e2a0e01a1b28f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_46d3d72e05594991b86764c6ca9cddab","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2033/? [4:29:39&lt;00:00,  9.56s/pipeline]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3900239162f94103888bb0f3ea5db9a6"}},"12fb34a724574d298295d03b34f95ab9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bfb558a473e34d399bf5f25f9b49089d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"46d3d72e05594991b86764c6ca9cddab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3900239162f94103888bb0f3ea5db9a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"kOlWS3SwJ0CQ"},"source":["# Modeling and Evaluation and Final Results\n","\n","Used Tree-based Pipeline Optimization Tool (TPOT) to find the best model\n","\n","TPOT automates the entire Machine Learning pipeline and provides a best performing machine learning model.\n","\n","\n","- How TPOT uses Genetic Programming to select the best machine learning model\n","- Feature Selection\n","- Feature preprocessing\n","- Feature construction\n","- Model selection\n","- Hyperparameter Optimization\n","- The score is the sklearn.model_selection.cross_val_score which does a K-Folds  with  scoring = accuracy\n"]},{"cell_type":"markdown","metadata":{"id":"hCbgZU6Ata9m"},"source":["## Step 0: Loading Modules and Dataset\n","\n","This section is to load and modules and the original dataset from a CSV file into a dataframe\n","\n","### Modules"]},{"cell_type":"code","metadata":{"id":"Zk8asf00ta9p"},"source":["# importing  packages\n","import numpy as np\n","import pandas as pd\n","# import matplotlib\n","# import matplotlib.pyplot as plt\n","# import seaborn as sns\n","# import re\n","\n","# ## importing datetime class\n","# from datetime import datetime, timedelta"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pnKPfe9AZrm3","executionInfo":{"status":"ok","timestamp":1623438287617,"user_tz":240,"elapsed":1213,"user":{"displayName":"Mark Drummond","photoUrl":"","userId":"07935756411103236239"}},"outputId":"c9f8ed78-d4ad-49bb-8495-1c129357eb7a"},"source":["# XG Bost\n","import xgboost as xgb\n","from sklearn.metrics import mean_squared_error\n","\n","# sklearn\n","# import `logistic regression` model\n","from sklearn import model_selection\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","\n","\n","# import required packages for evaluating models\n","from sklearn import metrics\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.exceptions import DataConversionWarning\n","# Import your necessary dependencies\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import Ridge\n","# Load scikit's random forest classifier library\n","from sklearn.ensemble import RandomForestClassifier\n","\n","\n","# Visualize Boosting Trees and Feature Importance\n","import graphviz\n","\n","\n","# For support Vector Machines with Scikit-learn Example\n","# Import scikit-learn dataset library\n","from sklearn import datasets\n","#Import svm model\n","from sklearn import svm\n","#Import scikit-learn metrics module for accuracy calculation\n","from sklearn import metrics\n","\n","# Bagged Decision Trees for Classification - necessary dependencies\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# For Bagged Example Data clean up\n","from sklearn.impute import SimpleImputer \n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Voting Ensemble \n","#from sklearn.linear_model import LogisticRegression\n","#from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import VotingClassifier\n","\n","# imports for evaluations\n","from sklearn.naive_bayes import GaussianNB\n","\n","# balance the data\n","from imblearn.over_sampling import SMOTE\n","\n","# AdaBoost\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","# warnings\n","import warnings\n","\n","#clf = LogisticRegression(max_iter=2000)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"RsKf1Bf1-5jn"},"source":["### SMOTE AND VALUATION Functions"]},{"cell_type":"code","metadata":{"id":"EaaPq8p2PFgI"},"source":["## NEW VALUATION CODE\n","def RunNewEval(df_working,modelname='LogisticRegression',model_param='max_iter=2000'):\n","    '''\n","    This Function run a SMOTE on the pandas dataframe passed\n","    it assumes that the Trarget(y) is the last column of the dataframe\n","    and all of the columns are numeric\n","    Prints out the results\n","    Just to make it easier to runn multiple datasets against each other\n","    \n","    Required Imports:\n","    -\tfrom sklearn.naive_bayes import GaussianNB\n","    -\tfrom sklearn.model_selection import train_test_split\n","    - from sklearn import metrics\n","    INPUTS:\n","    ----\n","    - df_working : DataFrame to run against\n","    OUTPUT:\n","    ----\n","    - f1_score - bias, variance\n","    - ROC_AUC  - bias, variance\n","    ''' \n","    #Create x and Y \n","    working_values = df_working.values\n","    # Slice Out X and Y\n","    X = working_values[:,:-1]\n","    #### create a variable `y` which contains the last column in `reg_values`\n","    y = working_values[:,-1:]\n","    # Print Before Shape\n","    print(\"Shape before SMOTE  X:\",X.shape,\" y:\" ,y.shape)\n","    # resample/balance the data\n","    sm = SMOTE(random_state = 2021) \n","    X_res, y_res = sm.fit_sample(X, y) \n","    # Print shape after reshape\n","    print(\"Shape after SMOTE  X:\",X_res.shape,\" y:\" ,y_res.shape) \n","#    print(\"Working on averaged f1_score from 10-fold CV (default)\")\n","    f1_bias, f1_variance = my_evalNew(X_res, y_res, modelname, 10, 'f1')    \n","#    print(\"Working on averaged ROC_AUC from 10-fold CV\")\n","    roc_bias, roc_variance = my_evalNew(X_res, y_res, modelname, 10, 'roc_auc')\n","    print(\"Averaged F1 1-bias : \" , f1_bias, \" Variance : \", f1_variance)\n","    print(\"Averaged ROC_AUC 1-bias : \" , roc_bias, \" Variance : \", roc_variance)    \n","    return  f1_bias, f1_variance, roc_bias, roc_variance\n","\n","\n","\n","def my_evalNew(X, y,modelname='LogisticRegression', k=10, scoring = 'f1'):\n","    #def my_evalNew(X, y, classifer = clf, k=10, scoring = 'f1'):\n","    '''\n","    return evaluation results (f1-score or ROC_AUC). \n","    Built in k-fold evaluation.\n","    INPUTS:\n","    ----\n","    - X: features; DataFrame or Numpy ndarray;\n","    - y: target; DataFrame or Numpy ndarray;\n","    - classifier: any sklearn (or its add-on) based classifier\n","    - k: number of folds in cross validation\n","    - scoring: evaluation metric ('f1' default or 'roc_auc')\n","    OUTPUT:\n","    ----\n","    bias/variance score of selected metric.\n","    '''\n","    #print(\"Using model : \", modelname)\n","    if modelname == 'LogisticRegression':\n","        print(\"Using model:\", modelname, \"with :\",scoring)\n","        clf = LogisticRegression(max_iter=2000)\n","                  \n","    elif modelname == 'SVC':\n","        p_kernel='linear'\n","        print(\"Using model:\", modelname, \"with :\",scoring)\n","        clf = svm.SVC(kernel=p_kernel) # Linear Kernel\n","        \n","    elif modelname == 'RandomForest':\n","        p_max_depth=2\n","        p_test_size=.3\n","        p_random_state=2019\n","        print(\"Using model:\", modelname, \"with :\",scoring)\n","        clf = RandomForestClassifier(max_depth=p_max_depth, random_state=p_random_state)\n","          \n","    elif modelname == 'AdaBoost':\n","        seed = 7\n","        num_trees = 70\n","        print(\"Using model:\", modelname, \"with :\",scoring)\n","        clf = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n","          \n","    elif modelname == 'DecisionTreeClassifier':\n","        print(\"Using model:\", modelname, \"with :\",scoring)\n","        clf = DecisionTreeClassifier()\n","\n","    elif modelname == 'XGBoost':\n","        p_objective ='reg:squarederror'\n","        p_colsample_bytree = 0.3\n","        p_learning_rate = 0.1\n","        p_max_depth = 5\n","        p_alpha = 10\n","        p_n_estimators = 100\n","        print(\"Using model:\", modelname, \"with :\",scoring)\n","        clf = xgb.XGBRegressor(objective = p_objective\n","                               , colsample_bytree = p_colsample_bytree\n","                               , learning_rate = p_learning_rate\n","                               , max_depth = p_max_depth\n","                               , alpha = p_alpha\n","                               , n_estimators = p_n_estimators)\n","\n","    elif modelname == 'GNaiveBayes':\n","        print(\"Using model:\", modelname, \"with :\",scoring)\n","        clf = GaussianNB()\n","\n","\n","    scores = []\n","    for i in range(2):\n","       #### generate random numbers to shuffle the data for training and test\n","       np.random.seed(2021)\n","       random_int = np.random.randint(0,3000)\n","       #### create cross validation folds\n","       kfold = model_selection.KFold(n_splits=k, random_state=random_int, shuffle=True)\n","       #### record the score\n","       score = model_selection.cross_val_score(clf, X=X, y=y, cv=kfold, scoring=scoring)\n","       scores.append(score)\n","    scores = np.array(scores)\n","    #### we need to calculate the bias (average score) and viariance (std)\n","    bias, variance = round(scores.mean(),4), round(scores.std(),4)\n","    return(bias, variance)\n","\n","def warn(*args, **kwargs):\n","    pass\n","\n","\n","\n","def CreateSmoteDF(df_working):\n","    '''\n","    This Function runs a SMOTE on the pandas dataframe passed\n","    it assumes that the Trarget(y) is the last column of the dataframe\n","    Return a balanced dataframe\n","    \n","    Required Imports:\n","    -\tfrom imblearn.over_sampling import SMOTE\n","    INPUTS:\n","    ----\n","    - df_working : DataFrame to run against\n","    OUTPUT:\n","    ----\n","    -  df_smote : DataFrame that has been balanced\n","    ''' \n","    #Create x and Y \n","    working_values = df_working.values\n","    # Slice Out X and Y\n","    X = working_values[:,:-1]\n","    #### create a variable `y` which contains the last column in `reg_values`\n","    y = working_values[:,-1:]\n","    df_column_lst = df_working.columns.to_list()\n","    # Print Before Shape\n","    print(\"Shape before SMOTE  X:\",X.shape,\" y:\" ,y.shape)\n","    # resample/balance the data\n","    sm = SMOTE(random_state = 2021) \n","    X_res, y_res = sm.fit_sample(X, y) \n","    # Print shape after reshape\n","    #print(\"Shape after SMOTE  X:\",X_res.shape,\" y:\" ,y_res.shape) \n","    # Recreate the Dataframe with the smote set\n","    df_smote = pd.concat([pd.DataFrame(X_res), pd.DataFrame(y_res)], axis=1)\n","    # rename the columns\n","    df_smote.columns = df_column_lst\n","    print(\"Shape after SMOTE  : \",df_smote.shape) \n","\n","    return df_smote\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zPxzAslnJ0CT"},"source":["# importing  packages\n","import numpy as np\n","import pandas as pd\n","# import matplotlib\n","# import matplotlib.pyplot as plt\n","# import seaborn as sns\n","# import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mST5XGMEJ0CW"},"source":["## Read-in Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"94SmLSq5J0CW","executionInfo":{"status":"ok","timestamp":1623438287754,"user_tz":240,"elapsed":17,"user":{"displayName":"Mark Drummond","photoUrl":"","userId":"07935756411103236239"}},"outputId":"ce586ff3-7841-4df1-ada5-e191a731efb4"},"source":["# Mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gYIVckCKuuu5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623438287755,"user_tz":240,"elapsed":15,"user":{"displayName":"Mark Drummond","photoUrl":"","userId":"07935756411103236239"}},"outputId":"784a38e4-d5ae-4d92-ce1a-01b44b95a9f4"},"source":["# \n","%%bash\n","ln -s drive/My\\ Drive/BUAN\\ 6590\\ -\\ Capstone/ Capstone"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ln: failed to create symbolic link 'Capstone/BUAN 6590 - Capstone': File exists\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324},"id":"ZDTW2a5kSTSd","executionInfo":{"status":"ok","timestamp":1623438288197,"user_tz":240,"elapsed":453,"user":{"displayName":"Mark Drummond","photoUrl":"","userId":"07935756411103236239"}},"outputId":"b9433a42-42c8-4625-8543-52bf22940d0e"},"source":["# Load dataset\n","data_file = '/content/Capstone/DATA/df_merge2_with_Target.csv'\n","df_data = pd.read_csv(data_file,index_col=0)\n","df_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>duration</th>\n","      <th>user_id</th>\n","      <th>steps</th>\n","      <th>floors</th>\n","      <th>intensity_minutes</th>\n","      <th>active_kilocalories</th>\n","      <th>hr_min</th>\n","      <th>hr_max</th>\n","      <th>hr_res</th>\n","      <th>stress_avg</th>\n","      <th>stress_dur_rest</th>\n","      <th>stress_dur_activity</th>\n","      <th>stress_dur_low</th>\n","      <th>stress_dur_medium</th>\n","      <th>stress_dur_high</th>\n","      <th>total_hours</th>\n","      <th>quality_hours</th>\n","      <th>spo2_minimum</th>\n","      <th>spo2_average</th>\n","      <th>deep_hours</th>\n","      <th>rem_hours</th>\n","      <th>Age</th>\n","      <th>survey_date</th>\n","      <th>ss7dmavg</th>\n","      <th>ss28dmavg</th>\n","      <th>ss28dStdev</th>\n","      <th>7D_StdDevfrom28d</th>\n","      <th>7Dssma0-1STDev_False</th>\n","      <th>7Dssma0-1STDev_True</th>\n","      <th>7Dssma1-2STDev_False</th>\n","      <th>7Dssma1-2STDev_True</th>\n","      <th>7Dssma2-3STDev_False</th>\n","      <th>7Dssma2-3STDev_True</th>\n","      <th>7Dssma3+STDev_False</th>\n","      <th>7Dssma3+STDev_True</th>\n","      <th>Status</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020-05-13</td>\n","      <td>86400</td>\n","      <td>0Sq4rLw6hryK3GlUpE6n</td>\n","      <td>7798</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>175</td>\n","      <td>42.0</td>\n","      <td>131.0</td>\n","      <td>52.0</td>\n","      <td>37.0</td>\n","      <td>22140.0</td>\n","      <td>12240.0</td>\n","      <td>14760.0</td>\n","      <td>16200.0</td>\n","      <td>4440.0</td>\n","      <td>5.25</td>\n","      <td>2.97</td>\n","      <td>88.0</td>\n","      <td>95.38</td>\n","      <td>1.45</td>\n","      <td>1.07</td>\n","      <td>68.0</td>\n","      <td>2020-05-19</td>\n","      <td>37.000000</td>\n","      <td>37.000000</td>\n","      <td>0.000001</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2020-05-14</td>\n","      <td>86400</td>\n","      <td>0Sq4rLw6hryK3GlUpE6n</td>\n","      <td>7787</td>\n","      <td>11.0</td>\n","      <td>0</td>\n","      <td>178</td>\n","      <td>37.0</td>\n","      <td>100.0</td>\n","      <td>54.0</td>\n","      <td>45.0</td>\n","      <td>15420.0</td>\n","      <td>20340.0</td>\n","      <td>12720.0</td>\n","      <td>19920.0</td>\n","      <td>5340.0</td>\n","      <td>6.45</td>\n","      <td>3.53</td>\n","      <td>84.0</td>\n","      <td>92.06</td>\n","      <td>1.45</td>\n","      <td>1.07</td>\n","      <td>68.0</td>\n","      <td>2020-05-19</td>\n","      <td>41.000000</td>\n","      <td>41.000000</td>\n","      <td>4.000000</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2020-05-15</td>\n","      <td>86400</td>\n","      <td>0Sq4rLw6hryK3GlUpE6n</td>\n","      <td>6432</td>\n","      <td>8.0</td>\n","      <td>0</td>\n","      <td>134</td>\n","      <td>48.0</td>\n","      <td>104.0</td>\n","      <td>52.0</td>\n","      <td>43.0</td>\n","      <td>19860.0</td>\n","      <td>15660.0</td>\n","      <td>13560.0</td>\n","      <td>17100.0</td>\n","      <td>8880.0</td>\n","      <td>5.93</td>\n","      <td>1.67</td>\n","      <td>87.0</td>\n","      <td>93.78</td>\n","      <td>1.45</td>\n","      <td>1.07</td>\n","      <td>68.0</td>\n","      <td>2020-05-19</td>\n","      <td>41.666667</td>\n","      <td>41.666667</td>\n","      <td>3.399346</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2020-05-16</td>\n","      <td>86400</td>\n","      <td>0Sq4rLw6hryK3GlUpE6n</td>\n","      <td>6682</td>\n","      <td>5.0</td>\n","      <td>0</td>\n","      <td>253</td>\n","      <td>49.0</td>\n","      <td>111.0</td>\n","      <td>52.0</td>\n","      <td>53.0</td>\n","      <td>12600.0</td>\n","      <td>15600.0</td>\n","      <td>9240.0</td>\n","      <td>17940.0</td>\n","      <td>14040.0</td>\n","      <td>7.58</td>\n","      <td>3.77</td>\n","      <td>84.0</td>\n","      <td>93.16</td>\n","      <td>1.45</td>\n","      <td>1.07</td>\n","      <td>68.0</td>\n","      <td>2020-05-19</td>\n","      <td>44.500000</td>\n","      <td>44.500000</td>\n","      <td>5.722762</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2020-05-17</td>\n","      <td>86400</td>\n","      <td>0Sq4rLw6hryK3GlUpE6n</td>\n","      <td>5406</td>\n","      <td>8.0</td>\n","      <td>0</td>\n","      <td>175</td>\n","      <td>52.0</td>\n","      <td>108.0</td>\n","      <td>58.0</td>\n","      <td>66.0</td>\n","      <td>4800.0</td>\n","      <td>24060.0</td>\n","      <td>6000.0</td>\n","      <td>11220.0</td>\n","      <td>24060.0</td>\n","      <td>5.60</td>\n","      <td>4.02</td>\n","      <td>83.0</td>\n","      <td>95.79</td>\n","      <td>1.45</td>\n","      <td>1.07</td>\n","      <td>68.0</td>\n","      <td>2020-05-19</td>\n","      <td>48.800000</td>\n","      <td>48.800000</td>\n","      <td>10.007997</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         date  duration  ... 7Dssma3+STDev_True  Status\n","0  2020-05-13     86400  ...                  0       2\n","1  2020-05-14     86400  ...                  0       2\n","2  2020-05-15     86400  ...                  0       2\n","3  2020-05-16     86400  ...                  0       2\n","4  2020-05-17     86400  ...                  0       2\n","\n","[5 rows x 37 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"zHPBGktyJ0Cm"},"source":["It's generally a good idea to randomly **shuffle** the data before starting to avoid any type of ordering in the data. You can rearrange the data in the DataFrame using numpy's **random** and **permutation()** function. To reset the index numbers after the shuffle use **reset_index()** method with **drop = True** as a parameter."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324},"id":"zi1MhbJ8J0Cn","executionInfo":{"status":"ok","timestamp":1623438288199,"user_tz":240,"elapsed":18,"user":{"displayName":"Mark Drummond","photoUrl":"","userId":"07935756411103236239"}},"outputId":"1eef3ecb-1107-49e5-e0b4-76ae52afbbf0"},"source":["df_data_shuffle=df_data.iloc[np.random.permutation(len(df_data))]\n","df_data2=df_data_shuffle.reset_index(drop=True)\n","df_data2.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>duration</th>\n","      <th>user_id</th>\n","      <th>steps</th>\n","      <th>floors</th>\n","      <th>intensity_minutes</th>\n","      <th>active_kilocalories</th>\n","      <th>hr_min</th>\n","      <th>hr_max</th>\n","      <th>hr_res</th>\n","      <th>stress_avg</th>\n","      <th>stress_dur_rest</th>\n","      <th>stress_dur_activity</th>\n","      <th>stress_dur_low</th>\n","      <th>stress_dur_medium</th>\n","      <th>stress_dur_high</th>\n","      <th>total_hours</th>\n","      <th>quality_hours</th>\n","      <th>spo2_minimum</th>\n","      <th>spo2_average</th>\n","      <th>deep_hours</th>\n","      <th>rem_hours</th>\n","      <th>Age</th>\n","      <th>survey_date</th>\n","      <th>ss7dmavg</th>\n","      <th>ss28dmavg</th>\n","      <th>ss28dStdev</th>\n","      <th>7D_StdDevfrom28d</th>\n","      <th>7Dssma0-1STDev_False</th>\n","      <th>7Dssma0-1STDev_True</th>\n","      <th>7Dssma1-2STDev_False</th>\n","      <th>7Dssma1-2STDev_True</th>\n","      <th>7Dssma2-3STDev_False</th>\n","      <th>7Dssma2-3STDev_True</th>\n","      <th>7Dssma3+STDev_False</th>\n","      <th>7Dssma3+STDev_True</th>\n","      <th>Status</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2021-01-03</td>\n","      <td>86400</td>\n","      <td>PT4Wz6SVCxEXSj4O8IfB</td>\n","      <td>3813</td>\n","      <td>6.0</td>\n","      <td>0</td>\n","      <td>97</td>\n","      <td>56.0</td>\n","      <td>101.0</td>\n","      <td>62.0</td>\n","      <td>14.0</td>\n","      <td>59520.0</td>\n","      <td>16560.0</td>\n","      <td>3420.0</td>\n","      <td>2340.0</td>\n","      <td>120.0</td>\n","      <td>10.03</td>\n","      <td>3.47</td>\n","      <td>84.0</td>\n","      <td>92.26</td>\n","      <td>1.05</td>\n","      <td>2.42</td>\n","      <td>25.0</td>\n","      <td>2021-01-05</td>\n","      <td>18.142857</td>\n","      <td>25.000000</td>\n","      <td>7.609518</td>\n","      <td>-0.901127</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2021-02-26</td>\n","      <td>86400</td>\n","      <td>B6XOaByr9nTwVK2vT8YI</td>\n","      <td>7682</td>\n","      <td>20.0</td>\n","      <td>0</td>\n","      <td>535</td>\n","      <td>44.0</td>\n","      <td>128.0</td>\n","      <td>52.0</td>\n","      <td>33.0</td>\n","      <td>27000.0</td>\n","      <td>22740.0</td>\n","      <td>13980.0</td>\n","      <td>10680.0</td>\n","      <td>3420.0</td>\n","      <td>7.63</td>\n","      <td>4.65</td>\n","      <td>83.0</td>\n","      <td>89.81</td>\n","      <td>1.78</td>\n","      <td>2.87</td>\n","      <td>29.0</td>\n","      <td>2021-03-02</td>\n","      <td>35.000000</td>\n","      <td>36.071429</td>\n","      <td>6.485856</td>\n","      <td>-0.165195</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2020-08-30</td>\n","      <td>86400</td>\n","      <td>zl7BTPWIYwuysZi5gVrm</td>\n","      <td>8461</td>\n","      <td>18.0</td>\n","      <td>0</td>\n","      <td>306</td>\n","      <td>53.0</td>\n","      <td>109.0</td>\n","      <td>56.0</td>\n","      <td>39.0</td>\n","      <td>19380.0</td>\n","      <td>21240.0</td>\n","      <td>15780.0</td>\n","      <td>14580.0</td>\n","      <td>4740.0</td>\n","      <td>9.43</td>\n","      <td>2.88</td>\n","      <td>83.0</td>\n","      <td>92.85</td>\n","      <td>1.45</td>\n","      <td>1.07</td>\n","      <td>66.0</td>\n","      <td>2020-09-01</td>\n","      <td>47.714286</td>\n","      <td>46.714286</td>\n","      <td>7.160692</td>\n","      <td>0.139651</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2020-06-23</td>\n","      <td>86400</td>\n","      <td>6gDGMpyGahFZYdhW8SUB</td>\n","      <td>10265</td>\n","      <td>65.0</td>\n","      <td>0</td>\n","      <td>722</td>\n","      <td>43.0</td>\n","      <td>110.0</td>\n","      <td>46.0</td>\n","      <td>40.0</td>\n","      <td>19860.0</td>\n","      <td>19860.0</td>\n","      <td>11880.0</td>\n","      <td>10800.0</td>\n","      <td>8880.0</td>\n","      <td>7.50</td>\n","      <td>1.98</td>\n","      <td>84.0</td>\n","      <td>92.26</td>\n","      <td>1.45</td>\n","      <td>1.07</td>\n","      <td>56.0</td>\n","      <td>2020-06-23</td>\n","      <td>44.142857</td>\n","      <td>43.464286</td>\n","      <td>7.688034</td>\n","      <td>0.088263</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2021-03-04</td>\n","      <td>86400</td>\n","      <td>8Xay1Wtk5twkSadg4hqm</td>\n","      <td>6533</td>\n","      <td>29.0</td>\n","      <td>23</td>\n","      <td>342</td>\n","      <td>56.0</td>\n","      <td>116.0</td>\n","      <td>64.0</td>\n","      <td>22.0</td>\n","      <td>46560.0</td>\n","      <td>13740.0</td>\n","      <td>10260.0</td>\n","      <td>7260.0</td>\n","      <td>1080.0</td>\n","      <td>7.85</td>\n","      <td>4.01</td>\n","      <td>89.0</td>\n","      <td>94.28</td>\n","      <td>1.58</td>\n","      <td>2.43</td>\n","      <td>33.0</td>\n","      <td>2021-03-09</td>\n","      <td>25.285714</td>\n","      <td>34.464286</td>\n","      <td>9.271624</td>\n","      <td>-0.989964</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         date  duration  ... 7Dssma3+STDev_True  Status\n","0  2021-01-03     86400  ...                  0       1\n","1  2021-02-26     86400  ...                  0       1\n","2  2020-08-30     86400  ...                  0       1\n","3  2020-06-23     86400  ...                  0       1\n","4  2021-03-04     86400  ...                  0       1\n","\n","[5 rows x 37 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"c6io8HP7J0Cu"},"source":["data_class = df_data2['Status'].values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xLkpxruCJ0Cv"},"source":["### MIssing Value Handling\n","\n","You should also do missing value treatment before using *tpot*. To check the number of missing values column-wise, you can execute the following:"]},{"cell_type":"code","metadata":{"id":"gsVrleHgz7jx"},"source":["# Drop unused columns in Actiity\n","drop_col = ['date','user_id','survey_date']\n","df_data2 = df_data2.drop(drop_col, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yDzDS8WazdQe","executionInfo":{"status":"ok","timestamp":1623438296934,"user_tz":240,"elapsed":142,"user":{"displayName":"Mark Drummond","photoUrl":"","userId":"07935756411103236239"}},"outputId":"8f134fde-ad42-4e16-dd0c-833c22f82058"},"source":["df_data2.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 37109 entries, 0 to 37108\n","Data columns (total 34 columns):\n"," #   Column                Non-Null Count  Dtype  \n","---  ------                --------------  -----  \n"," 0   duration              37109 non-null  int64  \n"," 1   steps                 37109 non-null  int64  \n"," 2   floors                37109 non-null  float64\n"," 3   intensity_minutes     37109 non-null  int64  \n"," 4   active_kilocalories   37109 non-null  int64  \n"," 5   hr_min                37109 non-null  float64\n"," 6   hr_max                37109 non-null  float64\n"," 7   hr_res                37109 non-null  float64\n"," 8   stress_avg            37109 non-null  float64\n"," 9   stress_dur_rest       37109 non-null  float64\n"," 10  stress_dur_activity   37109 non-null  float64\n"," 11  stress_dur_low        37109 non-null  float64\n"," 12  stress_dur_medium     37109 non-null  float64\n"," 13  stress_dur_high       37109 non-null  float64\n"," 14  total_hours           37109 non-null  float64\n"," 15  quality_hours         37109 non-null  float64\n"," 16  spo2_minimum          37109 non-null  float64\n"," 17  spo2_average          37109 non-null  float64\n"," 18  deep_hours            37109 non-null  float64\n"," 19  rem_hours             37109 non-null  float64\n"," 20  Age                   37109 non-null  float64\n"," 21  ss7dmavg              37109 non-null  float64\n"," 22  ss28dmavg             37109 non-null  float64\n"," 23  ss28dStdev            37109 non-null  float64\n"," 24  7D_StdDevfrom28d      37109 non-null  float64\n"," 25  7Dssma0-1STDev_False  37109 non-null  int64  \n"," 26  7Dssma0-1STDev_True   37109 non-null  int64  \n"," 27  7Dssma1-2STDev_False  37109 non-null  int64  \n"," 28  7Dssma1-2STDev_True   37109 non-null  int64  \n"," 29  7Dssma2-3STDev_False  37109 non-null  int64  \n"," 30  7Dssma2-3STDev_True   37109 non-null  int64  \n"," 31  7Dssma3+STDev_False   37109 non-null  int64  \n"," 32  7Dssma3+STDev_True    37109 non-null  int64  \n"," 33  Status                37109 non-null  int64  \n","dtypes: float64(21), int64(13)\n","memory usage: 9.6 MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p9NR2JXpe0jJ"},"source":["pd.isna(df_data2).any()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-M9N8vfCJ0Cw"},"source":["pd.isnull(df_data2).any()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FL8F83gNkJmz"},"source":["# LOAD IN TPOT"]},{"cell_type":"markdown","metadata":{"id":"L6oj4kkkJ0C3"},"source":["Now it's time to use the **tpot** library to suggest us the best pipeline for this binary classification problem. To do so, you have to import **TPOTClassifier** class from the tpot library. Had this been a regression problem you would have imported **TPOTRegressor** class.\n","\n","**TPOTClassifier** has a wide variety of parameters, and you can read all about them here. But the most notable ones you must know are:\n","\n","- generations: Number of iterations to the run pipeline optimization process. The default is `100`.\n","- population_size: Number of individuals to retain in the genetic programming population every generation. The default is `100`.\n","- offspring_size: Number of offspring to produce in each genetic programming generation. The default is `100`.\n","- mutation_rate: Mutation rate for the genetic programming algorithm in the range `[0.0, 1.0]`. This parameter tells the GP algorithm how many pipelines to apply random changes to every generation. Default is `0.9`.\n","- crossover_rate: Crossover rate for the genetic programming algorithm in the range `[0.0, 1.0]`. This parameter tells the genetic programming algorithm how many pipelines to \"breed\" every generation.\n","- scoring: Function used to evaluate the quality of a given pipeline for the classification problem like `accuracy, average_precision, roc_auc, recall`, etc. The default is `accuracy`.\n","- cv: Cross-validation strategy used when evaluating pipelines. The default is `5`.\n","- random_state: The seed of the pseudo-random number generator used in TPOT. Use this parameter to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.\n","\n","Also note mutation_rate + crossover_rate cannot exceed **1.0**.\n","\n","Here you will use tpot with generations = 5 and the rest of the parameters at default values. The parameter verbosity = 2 states how much information TPOT communicates while it's running.\n","\n","Then you will call the `fit()` method with the training set (without the target column) and the target column as the arguments.\n","\n","Note running the code in the below cell will take several hours to finish. With the given TPOT settings (5 generations with 100 population size), TPOT will evaluate 500 pipeline configurations before finishing. To put this number into context, think about a grid search of 500 hyperparameter combinations for a machine learning algorithm and how long that grid search will take. That is 500 model configurations to evaluate with 5-fold cross-validation, which means that roughly 2500 models are fit and evaluated on the training data in one grid search. That's a time-consuming procedure! Later, you will get to know about some more arguments that you can pass to TPOTClassifier to control the execution time for TPOT to finish.\n","\n","__NOTE__: be careful running this step - takes ~2 hours to run!"]},{"cell_type":"code","metadata":{"id":"n76X7LrmKn4L"},"source":["#### install TPOT\n","!pip install tpot"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KJAdZaEtjs3F"},"source":["## ORIGINAL Dataframe Not SMOTE"]},{"cell_type":"markdown","metadata":{"id":"bHCCtk8KJ0Cy"},"source":["You will now split the DataFrame into a training set and a testing set just like you do while doing any type of machine learning modeling. You can do this via sklearn's **cross_validation** **train_test_split**. The parameters are tele.index as indexes of the DataFrame, *train_size = 0.75* to keep 75% of the data in training set, *test_size = 0.25* to keep the rest 25% data in testing set and stratify = tele_class the class label's values in the dataset. Note the validation set is just to give us an idea of the test set error. Here it is kept to be the same as a test set."]},{"cell_type":"code","metadata":{"id":"BXHzw1SjJ0Cy"},"source":["from sklearn.model_selection import train_test_split\n","training_indices, testing_indices = train_test_split(df_data2.index,\n","                                                        stratify = data_class,\n","                                                        train_size=0.75, test_size=0.25, random_state = 2019)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ntNQ4-CkJ0C0"},"source":["You can check the size of the training set and validation set using the size attribute."]},{"cell_type":"code","metadata":{"id":"sKQlGFLiJ0C1"},"source":["training_indices.size, testing_indices.size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTFtQBPkJ0C3"},"source":["# from tpot import TPOTClassifier\n","# # from tpot import TPOTRegressor # for regression tasks\n","\n","# tpot = TPOTClassifier(generations=5,verbosity=2, n_jobs=-1)\n","\n","# tpot.fit(df_data2.drop('Status',axis=1).loc[training_indices].values, # X_train\n","#          df_data2.loc[training_indices,'Status'].values) # y_train\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HpCVnFv-LZWj"},"source":["# tpot.export('/content/Capstone/tpot_exported_pipeline.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWiOMiM1C4hg"},"source":["Optimization Progress: 43%\n","258/600 [3:20:28<6:33:05, 68.96s/pipeline]\n","\n","Generation 1 - Current best internal CV score: 0.7045021413272776"]},{"cell_type":"markdown","metadata":{"id":"b_PuFq4KJ0C5"},"source":["In the above only got to finish once.\n","The best pipeline is the one that has the CV accuracy score of **70.45%**. \n","\n","One of the key difference here is we use both `X_test` and `y_test` in the code below, since the `.score()` method below combines the __prediction__ and __evaluation__ in the same step."]},{"cell_type":"code","metadata":{"id":"qmumU-1sCZ7R"},"source":["# from tpot import TPOTClassifier\n","# # from tpot import TPOTRegressor # for regression tasks\n","\n","# tpot = TPOTClassifier(generations=5,verbosity=2, n_jobs=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZPssRymIJ0C5"},"source":["# tpot.score(df_data2.drop('Status',axis=1).loc[testing_indices].values, #X_test\n","#            df_data2.loc[testing_indices, 'Status'].values) # y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dCJNkLHsJ0C7"},"source":["As can be seen, the test accuracy is **89.16%.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zEy3t7mKJ0C7"},"source":["Isn't that awesome? Without you tweaking a lot of parameters and options to get the best model, TPOT not only gave you the information about the best model but also a working code for it!\n","\n","As indicated earlier, the last TPOT run took *several hours* to finish. Well, there are certain parameters you can specify to control the execution time of TPOT but with a trade-off. Since you will be limiting the time of TPOT execution, TPOT won't be able to explore all the possible pipelines and hence the best model suggested by TPOT at the end of the constrained time limit may not be the best model possible for that dataset. \n","\n","However, if sufficient time is given it will be somewhat closer to the best possible model. Some parameters are:\n","\n","- **max_time_mins**: how many minutes TPOT has to optimize the pipeline. If not None, this setting will override the generations parameter and allow TPOT to run until max_time_mins minutes elapse.\n","- **max_eval_time_mins**: how many minutes TPOT has to evaluate a single pipeline. Setting this parameter to higher values will enable TPOT to evaluate more complex pipelines, but will also allow TPOT to run longer. Use this parameter to help prevent TPOT from wasting time on assessing time-consuming pipelines. The default is 5.\n","- **early_stop**: how many generations TPOT checks whether there is no improvement in the optimization process. Ends the optimization process if there is no improvement in the given number of generations.\n","- **n_jobs**: Number of procedures to use in parallel for evaluating pipelines during the TPOT optimization process. Setting n_jobs=-1 will use as many cores as available on the computer. Beware that using multiple methods on the same machine may cause memory issues for large datasets. The default is 1.\n","- **subsample**: Fraction of training samples that are used during the TPOT optimization process. Must be in the range (0.0, 1.0]. The default is 1.\n","\n","Just for practice, you will again run TPOT with additional arguments `max_time_mins = 10` and `max_eval_time_mins = 0.4` but this time with reduced `population_size = 15`. I also setup an early stopping rule - if the model performance does not improve in `10` consecutive generations, the training process will stop.\n","\n","If you do not want to wait hours for your TPOT model, this might be the way to go!"]},{"cell_type":"code","metadata":{"id":"hUH4u2cmJ0C8"},"source":["from tpot import TPOTClassifier\n","tpot = TPOTClassifier(verbosity=2, max_time_mins=180, \n","                      max_eval_time_mins=0.4, population_size=15, early_stop=10, n_jobs=-1)\n","tpot.fit(df_data2.drop('Status',axis=1).loc[training_indices].values, # X_train\n","         df_data2.loc[training_indices,'Status'].values) # y_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pr2LoS8N3bt-"},"source":["tpot.score(df_data2.drop('Status',axis=1).loc[testing_indices].values, #X_test\n","           df_data2.loc[testing_indices, 'Status'].values) # y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WHce6Mlxdvtg"},"source":["tpot.export('/content/Capstone/tpot_exported_pipeline_two.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HFO9Q02tFFZH"},"source":["### 0.6832291442121147\n","\n","\n","\n","Best pipeline: DecisionTreeClassifier(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), criterion=gini, max_depth=7, min_samples_leaf=2, min_samples_split=7)\n","TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n","               disable_update_check=False, early_stop=10, generations=100,\n","               log_file=None, max_eval_time_mins=0.4, max_time_mins=180,\n","               memory=None, mutation_rate=0.9, n_jobs=-1, offspring_size=None,\n","               periodic_checkpoint_folder=None, population_size=15,\n","               random_state=None, scoring=None, subsample=1.0, template=None,\n","               use_dask=False, verbosity=2, warm_start=False)\n","\n","\n","# ***********"]},{"cell_type":"markdown","metadata":{"id":"00W9SeYmJ0C9"},"source":["As you can notice the best performing classifier within the time frame specified is DecisionTreeClassifier  with `CombineDFs()` and `CombineDFs()` as the pre-processing steps. \n","\n","After you trained your best model, you can always export the pipeline as a file and use it without any training (we know training takes a lot of time).\n","\n","You can export the above trained pipeline as (assume you have a `model` subfolder in your repo):\n","```python\n","tpot.export('tpot_exported_pipeline.py')\n","```\n","\n","Then in your subsequent analysis, you can import this `.py` file and then use the `tpot.score()` method to evaluate/deploy the model on _new, unseen_ data.\n","\n","For more examples of using TPOT for machine learning, refer to [these examples](https://epistasislab.github.io/tpot/examples/)."]},{"cell_type":"markdown","metadata":{"id":"kiHck9yNFdkA"},"source":["## TEST EXPORT"]},{"cell_type":"code","metadata":{"id":"-ocl3YbgFjWd"},"source":["# from tpot import TPOTClassifier\n","# tpot = TPOTClassifier(verbosity=2, max_time_mins=180, \n","#                       max_eval_time_mins=0.4, population_size=15, early_stop=3, n_jobs=-1)\n","# tpot.fit(df_data2.drop('Status',axis=1).loc[training_indices].values, # X_train\n","#          df_data2.loc[training_indices,'Status'].values) # y_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CY58OYIyFfz8"},"source":["tpot.export('/content/Capstone/tpot_exported_pipeline_two.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qyzFxi-9cTwn"},"source":["# SMOTE Unbalanced Target and rerun"]},{"cell_type":"markdown","metadata":{"id":"DiHmHvLOj6Lw"},"source":["## RUN WITH SMOTE Dataframe"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u8pNqW74cbY9","executionInfo":{"status":"ok","timestamp":1623438313473,"user_tz":240,"elapsed":1128,"user":{"displayName":"Mark Drummond","photoUrl":"","userId":"07935756411103236239"}},"outputId":"821de767-46b8-482a-f7f5-689064e1e7b0"},"source":["df_dataSM = CreateSmoteDF(df_data2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape before SMOTE  X: (37109, 33)  y: (37109, 1)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Shape after SMOTE  :  (100176, 34)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ApvFjwgKk-oD"},"source":["data_classSM = df_dataSM['Status'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6v5g_P8iw0H"},"source":["from sklearn.model_selection import train_test_split\n","training_indicesSM, testing_indicesSM = train_test_split(df_dataSM.index,\n","                                                        stratify = data_classSM,\n","                                                        train_size=0.75, test_size=0.25, random_state = 2019)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZ9DUoJdi4H7","executionInfo":{"status":"ok","timestamp":1623445362638,"user_tz":240,"elapsed":140,"user":{"displayName":"Mark Drummond","photoUrl":"","userId":"07935756411103236239"}},"outputId":"c3b46d23-eb9a-49a7-c38a-52125e4f7d22"},"source":["training_indicesSM.size, testing_indicesSM.size"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(75132, 25044)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["10866add10fc4adb891eba667a875352","a76f0874e9ad4632969bab26eafb0c28","a2ad146acdd24d3f9547d156889d7de3","1725c42bac7842699d8e2a0e01a1b28f","12fb34a724574d298295d03b34f95ab9","bfb558a473e34d399bf5f25f9b49089d","46d3d72e05594991b86764c6ca9cddab","3900239162f94103888bb0f3ea5db9a6"]},"id":"Ecb1StiejJMM","executionInfo":{"status":"ok","timestamp":1623461589185,"user_tz":240,"elapsed":16181831,"user":{"displayName":"Mark Drummond","photoUrl":"","userId":"07935756411103236239"}},"outputId":"52aed378-da13-4343-b065-436ed9e3e018"},"source":["from tpot import TPOTClassifier\n","#tpot = TPOTClassifier(verbosity=2, max_time_mins=180, \n","#                      max_eval_time_mins=0.4, population_size=15, early_stop=10, n_jobs=-1)\n","tpot = TPOTClassifier(verbosity=2, max_time_mins=None,                       \n","                      max_eval_time_mins=0.4, population_size=15,n_jobs=-1)\n","tpot.fit(df_dataSM.drop('Status',axis=1).loc[training_indicesSM].values, # X_train\n","         df_dataSM.loc[training_indicesSM,'Status'].values) # y_train"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10866add10fc4adb891eba667a875352","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=1515.0, style=ProgressStyle(dâ€¦"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Generation 1 - Current best internal CV score: 0.3690970335653245\n","\n","Generation 2 - Current best internal CV score: 0.3701085966640764\n","\n","Generation 3 - Current best internal CV score: 0.37639089516416036\n","\n","Generation 4 - Current best internal CV score: 0.37639089516416036\n","\n","Generation 5 - Current best internal CV score: 0.38151513884883426\n","\n","Generation 6 - Current best internal CV score: 0.38151513884883426\n","\n","Generation 7 - Current best internal CV score: 0.38151513884883426\n","\n","Generation 8 - Current best internal CV score: 0.38151513884883426\n","\n","Generation 9 - Current best internal CV score: 0.3888089588171169\n","\n","Generation 10 - Current best internal CV score: 0.38887550481363903\n","\n","Generation 11 - Current best internal CV score: 0.38887550481363903\n","\n","Generation 12 - Current best internal CV score: 0.4223100420219691\n","\n","Generation 13 - Current best internal CV score: 0.4223100420219691\n","\n","Generation 14 - Current best internal CV score: 0.4223100420219691\n","\n","Generation 15 - Current best internal CV score: 0.43886758659383157\n","\n","Generation 16 - Current best internal CV score: 0.43889420711825594\n","\n","Generation 17 - Current best internal CV score: 0.4411702070396363\n","\n","Generation 18 - Current best internal CV score: 0.4411835252736564\n","\n","Generation 19 - Current best internal CV score: 0.44123676455099226\n","\n","Generation 20 - Current best internal CV score: 0.44123676455099226\n","\n","Generation 21 - Current best internal CV score: 0.44130331054751426\n","\n","Generation 22 - Current best internal CV score: 0.456210276314294\n","\n","Generation 23 - Current best internal CV score: 0.46048282973960236\n","\n","Generation 24 - Current best internal CV score: 0.4605626930843883\n","\n","Generation 25 - Current best internal CV score: 0.4605626930843883\n","\n","Generation 26 - Current best internal CV score: 0.4617871132020042\n","\n","Generation 27 - Current best internal CV score: 0.4617871132020042\n","\n","Generation 28 - Current best internal CV score: 0.4618935988427273\n","\n","Generation 29 - Current best internal CV score: 0.4618935988427273\n","\n","Generation 30 - Current best internal CV score: 0.4618935988427273\n","\n","Generation 31 - Current best internal CV score: 0.4621997331020943\n","\n","Generation 32 - Current best internal CV score: 0.4707313268522711\n","\n","Generation 33 - Current best internal CV score: 0.4707313268522711\n","\n","Generation 34 - Current best internal CV score: 0.471077445929418\n","\n","Generation 35 - Current best internal CV score: 0.471077445929418\n","\n","Generation 36 - Current best internal CV score: 0.471077445929418\n","\n","Generation 37 - Current best internal CV score: 0.471077445929418\n","\n","Generation 38 - Current best internal CV score: 0.471077445929418\n","\n","Generation 39 - Current best internal CV score: 0.48237753790371085\n","\n","Generation 40 - Current best internal CV score: 0.4829232427107935\n","\n","Generation 41 - Current best internal CV score: 0.4829232427107935\n","\n","Generation 42 - Current best internal CV score: 0.4829232427107935\n","\n","Generation 43 - Current best internal CV score: 0.4833225576632101\n","\n","Generation 44 - Current best internal CV score: 0.4833225576632101\n","\n","Generation 45 - Current best internal CV score: 0.4833358758972303\n","\n","Generation 46 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 47 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 48 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 49 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 50 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 51 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 52 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 53 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 54 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 55 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 56 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 57 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 58 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 59 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 60 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 61 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 62 - Current best internal CV score: 0.48523921859238933\n","\n","Generation 63 - Current best internal CV score: 0.4854122812311104\n","\n","Generation 64 - Current best internal CV score: 0.4854122812311104\n","\n","Generation 65 - Current best internal CV score: 0.4854122812311104\n","\n","Generation 66 - Current best internal CV score: 0.4854122812311104\n","\n","Generation 67 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 68 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 69 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 70 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 71 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 72 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 73 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 74 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 75 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 76 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 77 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 78 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 79 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 80 - Current best internal CV score: 0.48574502184279844\n","\n","Generation 81 - Current best internal CV score: 0.48603797338888227\n","\n","Generation 82 - Current best internal CV score: 0.48603797338888227\n","\n","Generation 83 - Current best internal CV score: 0.48603797338888227\n","\n","Generation 84 - Current best internal CV score: 0.48603797338888227\n","\n","Generation 85 - Current best internal CV score: 0.48603797338888227\n","\n","Generation 86 - Current best internal CV score: 0.48603797338888227\n","\n","Generation 87 - Current best internal CV score: 0.48603797338888227\n","\n","Generation 88 - Current best internal CV score: 0.48603797338888227\n","\n","Generation 89 - Current best internal CV score: 0.48847347147466963\n","\n","Generation 90 - Current best internal CV score: 0.48847347147466963\n","\n","Generation 91 - Current best internal CV score: 0.48847347147466963\n","\n","Generation 92 - Current best internal CV score: 0.48847347147466963\n","\n","Generation 93 - Current best internal CV score: 0.7273065977137156\n","\n","Generation 94 - Current best internal CV score: 0.7273065977137156\n","\n","Generation 95 - Current best internal CV score: 0.7273065977137156\n","\n","Generation 96 - Current best internal CV score: 0.7273065977137156\n","\n","Generation 97 - Current best internal CV score: 0.7273065977137156\n","\n","Generation 98 - Current best internal CV score: 0.7273065977137156\n","\n","Generation 99 - Current best internal CV score: 0.7273065977137156\n","\n","Generation 100 - Current best internal CV score: 0.7273065977137156\n","\n","Best pipeline: KNeighborsClassifier(input_matrix, n_neighbors=17, p=2, weights=distance)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n","               disable_update_check=False, early_stop=None, generations=100,\n","               log_file=None, max_eval_time_mins=0.4, max_time_mins=None,\n","               memory=None, mutation_rate=0.9, n_jobs=-1, offspring_size=None,\n","               periodic_checkpoint_folder=None, population_size=15,\n","               random_state=None, scoring=None, subsample=1.0, template=None,\n","               use_dask=False, verbosity=2, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxBl8K6Ojgoz","executionInfo":{"status":"ok","timestamp":1623461595050,"user_tz":240,"elapsed":5886,"user":{"displayName":"Mark Drummond","photoUrl":"","userId":"07935756411103236239"}},"outputId":"05192fc4-1517-4fd1-c833-4c04fc216b27"},"source":["tpot.score(df_dataSM.drop('Status',axis=1).loc[testing_indicesSM].values, #X_test\n","           df_dataSM.loc[testing_indicesSM, 'Status'].values) # y_test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7466858329340361"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"C0zaahqi11Tk"},"source":["The results are better after balancing the data using SMOTE\n","The best classifier within the time frame specified is KNeighborsClassifier \n","\n","> ** 0.7466858329340361**\n"]},{"cell_type":"code","metadata":{"id":"xHUT-MpgjY43"},"source":["tpot.export('/content/Capstone/tpot_exported_pipeline_twoSMOTE2_7466_LAST.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UsIfmt450zhD"},"source":["# # Write out the merged Dataframe for use in other workbooks \n","# outPath = '/content/Capstone/DATA/df_dataSMOTE.csv'\n","# df_dataSM.to_csv(outPath)"],"execution_count":null,"outputs":[]}]}